<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Anas Brital


  | Decision Tree Algorithm Explained .

</title>
<meta name="description" content="This is my personal blog where I share my thoughts and what I've learned until now .
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®‚Äçüéì</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2021/Decision-Tree/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Anas Brital
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Decision Tree Algorithm Explained .</h1>
    <p class="post-meta">September 19, 2021</p>   
  </header>

  <article class="post-content">
    <p>Decision Trees is a supervised learning Algorithm used to solve classification and regression problems . The goal is to create a model that predicts the value of a target variable by learning simple decision rules taken from the data features.</p>

<div align="center">
<img src="/assets/img/19/decisionTree.png" width="300" height="200" />
</div>

<p>The idea behind Decision Tree is very simple , we try to find The best Feature and threshold that will split our data so that :</p>
<ul>
  <li>The samples that they‚Äôve feature value less than threshold will be stored in the left side .</li>
  <li>The samples that they‚Äôve a feature value more than threshold will be stored in the right side .</li>
  <li>if a node is leaf , we will store the value of the class which is the most common class in the samples in this node .</li>
</ul>

<p>We choose The Feature that will split our data Based on The Information gain , the feature that have to maximum value of the Information gain is the most Important.
to calculate the Information gain we need first to calculate The Entropy for every Feature , the Formula of the Entropy is defined as following:</p>

<div align="center">
<img src="/assets/img/19/Entropy.PNG" width="300" height="100" />
</div>

<p>Now we can Calculate The Information Gain using this Formula :</p>

<div align="center">
<img src="/assets/img/19/InformationGain.png" width="300" height="100" />
</div>
<h4 id="implementation-">Implementation :</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="k">class</span> <span class="nc">Node</span> <span class="p">:</span>
    
    <span class="s">"""
    Node Takes 5 parameters
        feature : the feature used to split in this Node
        threshold : the threshold used to split in this Node
        left : left Child of this Node
        right : right Child of This Node 
        value : the Common class in This Node in case if he's a leaf Node
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">feature</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="n">left</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feature</span> <span class="o">=</span> <span class="n">feature</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
    <span class="c1">#Function Used to verify if a node is leaf or Not    
</span>    <span class="k">def</span> <span class="nf">isLeafNode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
       <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

<span class="k">class</span> <span class="nc">DecisionTree</span> <span class="p">:</span>

    <span class="s">"""
    Decision Tree takes 3 parameters 
        min_Samples_to_Split : the Minimum Samples required to do a split 
        maximum_Depth : maximum depth of a tree
        nbr_features : the number of features in a tree
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">min_Samples_to_Split</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">,</span> <span class="n">maximum_Depth</span> <span class="o">=</span> <span class="mi">100</span> <span class="p">,</span> <span class="n">nbr_features</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">min_Samples_to_Split</span> <span class="o">=</span> <span class="n">min_Samples_to_Split</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">maximum_Depth</span> <span class="o">=</span> <span class="n">maximum_Depth</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nbr_features</span> <span class="o">=</span> <span class="n">nbr_features</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">root</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="c1">#Function Used to fit The Data to The Tree
</span>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">nbr_features</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">nbr_features</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">nbr_features</span> <span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">root</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">GrowTree</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> 
    
    <span class="c1">#a recursive Function used to add new Node based on a feature and a threshold to split
</span>    <span class="k">def</span> <span class="nf">GrowTree</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">nbr_samples</span> <span class="p">,</span> <span class="n">nbr_features</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="s">"""
            np.unique it a numpy array that gives as unique values in an array
            example :
                a = np.array([1,1,1,2,2,3])
                np.unique(a) return =&gt; [1,2,3]
        """</span>
        <span class="n">nbr_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        
        <span class="s">"""
        The Stop Criteria contains three conditions :
            * Number Of Samples less than minimum samples to split (in our case is 2).
            * depth greather than or equal the maximum depth (in our case is 100).
            * Number of classes in a node is 1 , so we don't need to split the data .
        """</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">nbr_samples</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">min_Samples_to_Split</span> <span class="ow">or</span> <span class="n">depth</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maximum_Depth</span> <span class="ow">or</span> <span class="n">nbr_classes</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">):</span>
            <span class="n">leafValue</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">getMostCommonClass</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">leafValue</span><span class="p">)</span>
        <span class="k">else</span> <span class="p">:</span>
            <span class="c1">#get The list of features in an array in a random way
</span>            <span class="n">features_Indexes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nbr_features</span> <span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">nbr_features</span> <span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
            <span class="c1">#get The Best Feature and The Best Threshold to split the data using Greedy Search
</span>            <span class="n">bestFeature</span> <span class="p">,</span> <span class="n">bestThreshold</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bestCriteria</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">features_Indexes</span><span class="p">)</span>
            <span class="c1">#Now we've our best Feature to split the data and The Best Threshold
</span>            <span class="n">leftIndexes</span> <span class="p">,</span> <span class="n">rightIndexes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">bestFeature</span><span class="p">]</span> <span class="p">,</span> <span class="n">bestThreshold</span><span class="p">)</span>
            <span class="c1">#call the function GrowTree recursively
</span>            <span class="n">leftChild</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">GrowTree</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">leftIndexes</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">leftIndexes</span><span class="p">]</span> <span class="p">,</span> <span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">rightChild</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">GrowTree</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">rightIndexes</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="n">rightIndexes</span><span class="p">]</span> <span class="p">,</span> <span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Node</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">bestFeature</span> <span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">bestThreshold</span> <span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="n">leftChild</span> <span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">rightChild</span> <span class="p">)</span>

        <span class="s">"""
        This Function used to get The Most Common label :
            a = np.array([1,1,1,2,2,3,2,1,2])
            common = Counter(a).most_common(1) =&gt; return ([1,4])
            to get The most Common lable we do this =&gt; common:Label = common[0][0]
        """</span>
    <span class="k">def</span> <span class="nf">getMostCommonClass</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Counter</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="s">"""
        This Function is Used To Get The Best Feature and The Best threshold to split The Data
        Using Greedy Search .
        """</span>
    <span class="k">def</span> <span class="nf">bestCriteria</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">featuresIndexes</span><span class="p">):</span>
        <span class="n">bestInformationGain</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">split_Index</span> <span class="p">,</span> <span class="n">split_Threshold</span> <span class="o">=</span> <span class="bp">None</span> <span class="p">,</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">FeatureIndex</span> <span class="ow">in</span> <span class="n">featuresIndexes</span><span class="p">:</span>
            <span class="n">xi</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span><span class="n">FeatureIndex</span><span class="p">]</span>
            <span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">thresh</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
                <span class="c1">#Calculate The Information Gain if we split using This Feature and this threshold
</span>                <span class="n">CurrentInformationGain</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">informationGain</span><span class="p">(</span><span class="n">xi</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">thresh</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">CurrentInformationGain</span> <span class="o">&gt;</span> <span class="n">bestInformationGain</span> <span class="p">:</span>
                    <span class="n">bestInformationGain</span> <span class="o">=</span> <span class="n">CurrentInformationGain</span>
                    <span class="n">split_Index</span> <span class="o">=</span> <span class="n">FeatureIndex</span>
                    <span class="n">split_Threshold</span> <span class="o">=</span> <span class="n">thresh</span>
        <span class="k">return</span> <span class="n">split_Index</span> <span class="p">,</span> <span class="n">split_Threshold</span>            
            
    <span class="s">"""
    This Function Used to calculate The Information Gain Using This Formula :
        Information_Gain = Entropy(Parent) - [Weighted Average of child i] * entropy(child i)
    """</span>
    <span class="k">def</span> <span class="nf">informationGain</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
        <span class="n">parentEntropy</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        
        <span class="c1">#Now we Need to split the data to find The left and right children
</span>        <span class="n">left_indexes</span> <span class="p">,</span> <span class="n">right_indexes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>        
        
        <span class="n">parentSize</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">leftSize</span> <span class="p">,</span> <span class="n">rightSize</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">left_indexes</span><span class="p">)</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">right_indexes</span><span class="p">)</span>
        <span class="n">leftWeightAVG</span> <span class="p">,</span> <span class="n">rightWeightAVG</span> <span class="o">=</span> <span class="n">leftSize</span> <span class="o">/</span> <span class="n">parentSize</span> <span class="p">,</span> <span class="n">rightSize</span> <span class="o">/</span> <span class="n">parentSize</span>
        <span class="n">leftEntropy</span> <span class="p">,</span> <span class="n">rightEntropy</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">left_indexes</span><span class="p">])</span> <span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">right_indexes</span><span class="p">])</span>
        <span class="c1">#Information_Gain = Entropy(Parent) - [Weighted Average of child i] * entropy(child i)
</span>        <span class="n">Information_Gain</span> <span class="o">=</span> <span class="n">parentEntropy</span> <span class="o">-</span> <span class="p">(</span><span class="n">leftWeightAVG</span> <span class="o">*</span> <span class="n">leftEntropy</span> <span class="o">+</span> <span class="n">rightWeightAVG</span> <span class="o">*</span> <span class="n">rightEntropy</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Information_Gain</span>
    <span class="s">"""
    This Function Used to Calculate The Entropy for a Node Using This Formula :
        entropy = - Sigma ( (probability of X) * log2 (probability of X) )
    """</span>    
    <span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">NumberOfOccurances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">NumberOfOccurances</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">([</span><span class="n">probability</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span> <span class="k">for</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">probabilities</span> <span class="k">if</span> <span class="n">probability</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">])</span>
    <span class="s">"""
    This Function Used to split The Data using a threshold passed in the parameters.
    """</span>     
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">thresh</span><span class="p">):</span>
        <span class="n">left_indexes</span> <span class="p">,</span> <span class="n">right_Indexes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="n">thresh</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span> <span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">left_indexes</span> <span class="p">,</span> <span class="n">right_Indexes</span>
    <span class="s">"""
    This Function Used To Predict a new Sample.
    """</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">traverseTree</span><span class="p">(</span><span class="n">xi</span> <span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">root</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>
    
    <span class="s">"""
    This Function Used to traverse The Tree .
    """</span>
    <span class="k">def</span> <span class="nf">traverseTree</span><span class="p">(</span><span class="bp">self</span> <span class="p">,</span> <span class="n">x</span> <span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">isLeafNode</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">node</span><span class="p">.</span><span class="n">value</span>
        
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">feature</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">node</span><span class="p">.</span><span class="n">threshold</span> <span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">traverseTree</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">left</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">traverseTree</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">right</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h4 id="testing-the-model-">Testing The Model :</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_true</span> <span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span> <span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

<span class="n">x</span> <span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span> <span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_train</span> <span class="p">,</span> <span class="n">x_test</span> <span class="p">,</span> <span class="n">y_train</span> <span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">decisionTree</span> <span class="o">=</span> <span class="n">DecisionTree</span><span class="p">(</span><span class="n">nbr_features</span><span class="o">=</span><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">decisionTree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">decisionTree</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Model Accuracy : "</span><span class="p">,</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

  </article>

</div>

    </div>
  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
